{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# What a Neighborhood Needs\n\n## Introduction\n\n### Background\nThere is a natural flow of customers between businesses. One might go from a hair salon to a nail salon.  One might stop for drinks after dancing.  Perhap after clothes shopping people like to grab coffee.  People will tend to go from one category of business to another. Ideally, near the one you will find the other.  You can find opportunity where this isn't the case.  For example if we can determine that people like to stop for bubble tea after shoe shopping, and we can find a neighborhood with shoe shop but without bubble tea, that might just be the right to invest in a new bubble tea shop.\n\n### Problem\nThere are few steps to this analysis. First we create a mapping of customer flow from one category of business to another.  Then we find geographic clusters of businesses to call our neighborhoods.  We can then analyze the categories and popularity of the businesses in each neighborhood to find out what categories of businesses would be expected popular next stops.  We then compare the expected hot categories to the categories existent in the neighborhood to find deficiencies. \n\n### Interest\nThe results of this analysis would be of interest to anyone considering starting a business, or investing in a new business and wants to know where existing customers will naturally feed into their business.  \n\n## Data\n### Data Source\nFor this project we restrict our area of interest to the city of Seattle, WA; my hometown.  We use Foursquare Places API https://developer.foursquare.com/docs/api to get information on various businesses. Foursquare has a remarkably complete dataset of businesses and other locations.  Specifically, we use Foursquare Places API `search` call to get a list of businesses and locations, called `venues` by the API. As the call is limited to 50 venues, we use the call in a tight grid across the city, aggregating and removing duplicates in the data.  The key pieces of information this gets us is the unique venue identifier, the primary category associated with the venue (e.g. \"German Restaurant\"), and the exact location. On each venue we get the number of likes with the `likes` call, and the categories of the next venues with the `nextvenue` call. The `nextvenue` call returns the 5 venues that are most commonly checked into immediately after checking into a given venue as long as 2 hours haven't passed between the checkins. \n\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import math\n\nimport pandas as pd\nimport numpy as np\nimport requests\n!pip install foursquare\nimport foursquare\nfrom project_lib import Project\nimport threading\nimport json\nimport pickle"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "To collect the full dataset, this notebook needs to be run once with `GET_FRESH_VENUES` and `CREATE_INITAL_DATAFRAME` set as `True` , and approximately 10 times with both set as `False`, each time space more than an hour apart. This is to work around the Foursquare hourly quota of 5000 regular calls. Getting the full dataset requires approximately 50000 total calls.  \n\nThis Notebook uses IBM Watson project object storage to store files.  If someone wanted to run this in a different environment than IBM Watson Studio, then the file storage would have to be adapted to the new environment. Simply search for the use of `project` and replace those lines as makes sense. \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "GET_FRESH_VENUES = False\nCREATE_INITAL_DATAFRAME = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We now prepare ourselves to use the Foursquare and Watson Project APIs"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The below hidden cell looks like  \n`CLIENT_ID = <My Foursquare Client ID>`  \n`CLIENT_SECRET = <My Foursquare Secret>`  \nwith `<My Foursquare Client ID>` and `<My Foursquare Secret>` replaced with the corresponding quoted values from my Foursquare developer account. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "VERSION = 20180605\nclient = foursquare.Foursquare(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, version=VERSION)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The hidden cell below looks like  \n`PROJECT_ID=<My Project ID>`  \n`PROJECT_ACCESS_TOKEN=<My Project Access Token>`  \nwith `<My Project ID>` and `<My Project Access Token>` replaced with my Watson Studio project's ID and it's access token respectively. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "project = Project(project_id=PROJECT_ID, project_access_token=PROJECT_ACCESS_TOKEN)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Getting the Venues\n\n#### Preliminary calculations\nWe call Foursquare `search` in a grid to gather all venues in Seattle.  To do this, first we figure out our grid of latitude and longitudes we will search on. We get the extrema of Seattle manually from Google Maps."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nseattle_north = 47.734401\nseattle_west = -122.437307\nseattle_south = 47.494613\nseattle_east = -122.245315"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We want to do foursquare searches of approximately 210 meters radius, with the expectation that it is unlikely that more than 50 venues\nwill be in any such tight space.  This is a guess, but we will double check as we do our searches that we don't max any out. We want to make sure to do a search grid that completely covers the area.  If we do a square grid, how much great a spacing will that allow? Let's reduce it by a bit just to be extra certain of no gaps. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "radius = 210\n\nspacing = 2*radius/(2**(1/2)) /1.05\nspacing\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "How many searches will that require? First we calculate the length in meters of a degree of longitude and latitude around Seattle, using\n40000000 as the approximate circumference of the earth in meters. At one time, this was true by the definition of the meter."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "degree_long = 40000000/360\ndegree_lat = degree_long * math.cos(math.pi*seattle_north/180)\n\nseattle_h = (seattle_north - seattle_south)*degree_long\nseattle_w = (seattle_east - seattle_west)* degree_lat\n\ngrid_h = int(seattle_h/spacing)\ngrid_w = int(seattle_w/spacing)\ngrid_h * grid_w"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We have a 5000 per hour limit on the number of API calls, it's important that the grid height multiplied by the grid width is below that. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we call `search` in each node in the grid,  we multithread the process to speed things up. We then save the results to permanent storage. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "\nVENUES_PATH = \"venues.json\"\n\nif GET_FRESH_VENUES:\n    def get_venues(lat, lng, venues_dict):\n        result = client.venues.search(params = {'ll':\"%f,%f\"%(lat, lng), 'radius':'100'})\n        venues_list = result[\"venues\"]\n        if len(venues_list) >= 50:\n            print(\"Found %d venues at %f, %f\"%(len(venues_list), search_lat, search_lng) )\n        for venue in result[\"venues\"]:\n            venues_dict[venue[\"id\"]] = venue\n    venues_dict = {} #We use a dictionary to remove duplicate entries\n    threads = []\n    for search_lat in np.linspace(seattle_south, seattle_north, grid_h):\n        print(\"Scanning Latitude %f\" % search_lat)\n        for search_lng in np.linspace(seattle_west, seattle_east, grid_w):\n            thread = threading.Thread(target=get_venues, args=(search_lat, search_lng, venues_dict))\n            thread.start()\n            threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n        \n    venues_json = json.dumps(venues_dict)\n    project.save_data(file_name = VENUES_PATH,data = venues_json,set_project_asset=True,overwrite=True)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Load from permanent storage if we didn't just regenerate them.  "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "if not GET_FRESH_VENUES:\n    venues_file = project.get_file(VENUES_PATH)\n    venues_dict = json.load(venues_file)\n    venues_file.close()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Creating our Dataframe\nWe extract the parameters of interest from our raw results, and place them in a dataframe.  We put in dummy values for \"likes\" and \"next categories\" to be filled in later. We also filter out any Venues without categories, or not in Seattle. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "DF_PATH = \"df.pickle\"\n\ndef primary_category(venue):\n    for cat in venue['categories']:\n        if cat['primary']:\n            return cat\n    return None\n\nif CREATE_INITAL_DATAFRAME:\n    columns = ['id', 'name', 'category id', 'category name', 'lat', 'lng', 'likes', 'next categories']\n    data = []\n    for venue in venues_dict.values():\n\n        cat = primary_category(venue)\n        # Strip out anything without a category or that isn't in Seattle\n        if (cat is not None and 'city' in venue['location'] \n                and venue['location']['city'] == 'Seattle'):\n            data.append((venue['id'], venue['name'], cat['id'], \n                 cat['name'], venue['location']['lat'], \n                 venue['location']['lng'], np.nan, np.nan))\n\n    df = pd.DataFrame(data=data, columns = columns).set_index('id')\n    df['next categories'] = df['next categories'].astype(np.object)\nelse:\n    df_file = project.get_file(DF_PATH)\n    df = pickle.load(df_file)\n    df_file.close()\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Likes and Next Categories\n\nHere we fill in \"likes\" and \"next categories\" values in our dataframe.  This can't be done all in one go because we have an hourly quota of Foursquare calls. As mentioned before, we can only make 5000 Foursquare calls an hour. We need to make approximately  40000.  So this notebook does what it can, and saves partial results.  The notebook is rerun with `GET_FRESH_VENUES` and `CREATE_INITAL_DATAFRAME` set as `False` with each run space more than an hour apart until it is complete. I do this with a project job scheduled to repeatedly run. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "for counter, venue_id in enumerate(df.index):\n    if counter%100 == 0:\n        print(\"At entry %d and rate remaining is %s\" % (counter, client.rate_remaining))\n    if np.isnan(df.at[venue_id, 'likes']):\n        df.at[venue_id, 'likes'] = client.venues.likes(venue_id)['likes']['count']\n    # isnan will error if run on a list.  Instead let's check if it is a list. \n    if isinstance(df.at[venue_id, 'next categories'], list):\n        next_cats = []\n        result = client.venues.nextvenues(venue_id)\n        for venue in result['nextVenues']['items']:\n            primary = primary_category(venue)\n            if primary is not None:\n                next_cats.append({'id': primary['id'],'name':primary['name']})\n        df.at[venue_id, 'next categories'] = next_cats\n    # Reserve our last 100 calls.  It's good never to be competely out\n    if client.rate_remaining is not None and int(client.rate_remaining) < 100:\n        break    "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "if df['next categories'].hasnans:\n    print(\"Not complete will need to be rerun after an hour.\")\nelse:\n    print(\"All data gathered.\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Save to permanent storage. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "project.save_data(file_name = DF_PATH,data = pickle.dumps(df),set_project_asset=True,overwrite=True)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "After all data is gathered, the analysis notebook can be run to analyze the data. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}